{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to extract regular expression patterns(strg) \n",
    "## form the given string(st)\n",
    "\n",
    "## We are here using spacy style regex matcher \n",
    "\n",
    "def regex_extractor(strg,doc,st):\n",
    "  a=[]\n",
    "  expression=strg\n",
    "  for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    b=[]\n",
    "    b.append(st[start:end])\n",
    "    b.append(start)\n",
    "    b.append(end)\n",
    "    a.append(b)\n",
    "  return(a)     # returning a list of list containing txt,start_ind,end_index\n",
    "                # of the matched pattern\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## string:- Original String\n",
    "## nlp:- spacy model trained on english web data\n",
    "## nlp2:- Retrained spacy en_core_web_sm model on medical data (check final_training_data file)\n",
    "## choice:- 1 for completing removing dates fron text except year.\n",
    "##          2 for shifting the dates to protect the information without loss of valuable information(more preffered)\n",
    "\n",
    "def deidentifier(string,nlp,nlp3,choice):\n",
    "    doc=nlp((open(string)).read())        ## spacy object containg processed string i.e string after passing through default en_core_web_sm spacy model.\n",
    "    st=open((string)).read()              ## st=original string\n",
    "    \n",
    "    \n",
    "    time=['YEAR', 'YEARS', 'AGE', 'AGES', 'MONTH', 'MONTHS', 'DECADE', 'CENTURY', 'WEEK', 'DAILY', 'DAY', 'DAYS', 'NIGHT', 'NIGHTS', 'WEEKLY', 'MONTHLY', 'YEARLY']\n",
    "    address_identifier=['st','niwas','aawas','palace','road','block','gali','sector','flr','floor','path','near','oppo','bazar','house','nagar','bypass','bhawan','street','rd','sq','flat','lane','gali','circle','bldg','ave','mandal','avenue','tower','nagar','marg','chowraha','lane','heights','plaza','park','garden','gate','villa','market','apartment','chowk']\n",
    "    \n",
    "    ## regex extractor gets a regex string,doc,original string and returns a list of list containing matched pattern along with\n",
    "    ## start and end index of the pattern.\n",
    "    date_list1=regex_extractor(r\"\\D([0-9]{4}|[0-9]{1,2})(\\/|-)[0-9]{1,2}(\\/|-)([0-9]{1,2}|[0-9]{4})\\D\",doc,st)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(date_list1)):\n",
    "        date_list1[i][1]=date_list1[i][1]+1\n",
    "        date_list1[i][2]=date_list1[i][2]-1\n",
    "        date_list1[i][0]=st[date_list1[i][1]:date_list1[i][2]]\n",
    "    \n",
    "    ## If the choice is 1(remove the dates except year) this part of the program executes.\n",
    "    \n",
    "    if(choice==1):\n",
    "        for a in date_list1:\n",
    "            count=0\n",
    "            for i in range(a[1],a[1]+4):\n",
    "                if(st[i].isnumeric()):\n",
    "                    count=count+1\n",
    "            if(count==4):\n",
    "                st=st[:a[1]+4]+'X'*(a[2]-a[1]-4)+st[a[2]:] \n",
    "            else:\n",
    "                count=0\n",
    "                for j in range(a[2],a[2]-5,-1):\n",
    "                    if(st[j].isnumeric()):\n",
    "                        count=count+1\n",
    "                #print(\"count\",count)        \n",
    "                if(count==4):\n",
    "                    #print(st[a[1]:a[2]])\n",
    "                    st=st[:a[1]]+'X'*(a[2]-4-a[1])+st[a[2]-4:]\n",
    "                elif(count==3):\n",
    "                    st=st[:a[1]]+'X'*(a[2]-2-a[1])+st[a[2]-2:]\n",
    "                else:\n",
    "                    st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "                    \n",
    "                    \n",
    "    ## If the choice is 2(shifting the dates by a value)  this part of code executes and returns a list of shifted dates\n",
    "    ## along with the shift encountered.\n",
    "    shifted_datelist=[]\n",
    "    temp=0\n",
    "    k=0\n",
    "    random_val=randint(0,60)   ## getting a random number to shift all the dates by that number\n",
    "    if(choice==2):\n",
    "        for kk in range(len(date_list1)):\n",
    "            llst=[]\n",
    "            text=date_list1[kk][0]\n",
    "            front=date_list1[kk][1]+k\n",
    "            back=date_list1[kk][2]+k \n",
    "        \n",
    "        ### to shift the dates we are convering all the dates to pandas datetime and then by using pandas timedelta \n",
    "        ##  we can get the shifted dates.\n",
    "            new_date=pd.to_datetime(text, infer_datetime_format=True,errors='ignore') \n",
    "            if(type(new_date)!=str):\n",
    "               # print(new_date)\n",
    "               # print(type(new_date))\n",
    "                new_date=new_date+timedelta(days=random_val)\n",
    "               # print(str(new_date)[:-9])\n",
    "                st=st[:front]+str(new_date)[:-9]+st[back:]\n",
    "                k=k+(len(str(new_date)[:-9])-len(text))\n",
    "                llst.append(str(new_date)[:-9])\n",
    "                llst.append(front)\n",
    "                llst.append(front+len(str(new_date)[:-9]))\n",
    "                shifted_datelist.append(llst)\n",
    "               # k=k+(10-len(text))\n",
    "    \n",
    "    ## function call to extract mail,ip address, and aadhar number form the text. \n",
    "    \n",
    "    mail_list=regex_extractor(r\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\",doc,st)    \n",
    "    ip_list=regex_extractor(r\"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\",doc,st)    \n",
    "    aadhar_list=regex_extractor(r\"(\\d{4}(\\s|\\-)\\d{4}(\\s|\\-)\\d{4})\",doc,st)\n",
    "    \n",
    "    ## replacing all the matched pattern to protect the information.\n",
    "    \n",
    "    for a in ip_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    for a in mail_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:] \n",
    "    for a in aadhar_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    ## Function call to extract urls and license plate numbers form the text\n",
    "    doc=nlp(st)\n",
    "    url_list=regex_extractor(r\"(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\",doc,st)    \n",
    "    license_plate_list=regex_extractor(r\"[A-Z]{2}[ -][0-9]{1,2}(?: [A-Z])?(?: [A-Z]*)? [0-9]{4}\",doc,st)  \n",
    "    for a in url_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    for a in license_plate_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    \n",
    "  ##  Hiding the extracted information to protect identity\n",
    "    \n",
    "    \n",
    "    ## Function call to extract phone numbers and fax numbers from the text.\n",
    "    doc=nlp(st)\n",
    "    phone_fax_list1=regex_extractor(r\"(?:(?:(?:(\\+)((?:[\\s.,-]*[0-9]*)*)(?:\\()?\\s?((?:[\\s.,-]*[0-9]*)+)(?:\\))?)|(?:(?:\\()?(\\+)\\s?((?:[\\s.,-]*[0-9]*)+)(?:\\))?))((?:[\\s.,-]*[0-9]+)+))\",doc,st)\n",
    "    phone_fax_list2=regex_extractor(r\"\\D(\\+91[\\-\\s]?)?[0]?(91)?[789]\\d{9}\\D\",doc,st)\n",
    "    for i in range(len(phone_fax_list2)):\n",
    "      phone_fax_list2[i][1]=phone_fax_list2[i][1]+1\n",
    "      phone_fax_list2[i][2]=phone_fax_list2[i][2]-1\n",
    "      phone_fax_list2[i][0]=st[phone_fax_list2[i][1]:phone_fax_list2[i][2]]\n",
    "    \n",
    "    phone_fax_list=[]\n",
    "    for a in phone_fax_list1:\n",
    "        phone_fax_list.append(a)\n",
    "    for a in phone_fax_list2:\n",
    "        phone_fax_list.append(a) \n",
    "    \n",
    "    for a in phone_fax_list1:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    for a in phone_fax_list2:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    \n",
    "    \n",
    "    ## Function call to extract pan number,passport number,account number,credit card number and medical record number and finally \n",
    "    ## hiding them to protect the information.\n",
    "    doc=nlp(st)\n",
    "    pan_list=regex_extractor(r\"[A-Z]{5}\\d{4}[A-Z]{1}\",doc,st)\n",
    "    passport_list=regex_extractor(r\"[A-Z]{1}\\d{7}\",doc,st)\n",
    "    account_and_serial_list=regex_extractor(r\"\\d{9,18}\",doc,st)\n",
    "    credit_card_list=regex_extractor(r\"\\d{5}(\\s|\\-)\\d{5}(\\s|\\-)\\d{5}|\\d{4}(\\s|\\-)\\d{4}(\\s|\\-)\\d{4}(\\s|\\-)\\d{4}\",doc,st)\n",
    "    medical_rep=regex_extractor('\\d{7}',doc,st)\n",
    "    ###  medical_report_no : Assuming the pattern to be 7 digit number as it is organisation dependent and can also be changed later\n",
    "    ###  accordingly.\n",
    "    \n",
    "    for a in account_and_serial_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    for a in pan_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    for a in passport_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    for a in credit_card_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    for a in medical_rep:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "\n",
    "   \n",
    "\n",
    "    ###  After extracting many useful information the below line of code extracts address fron the text.\n",
    "    ###  Address which are smaller than street name.\n",
    "    \n",
    "   ### For extracting addresses we use a list of common words used in addressing and match them with every elemnt in spacy \n",
    "   ### doc object and if any match is found it is inserted in addr list.\n",
    "\n",
    "    doc=nlp(st)\n",
    "    addr=[]\n",
    "    for i in doc:\n",
    "      if(len(i)>1 and '\\n' not in str(i)):\n",
    "           if(str(i).lower() in address_identifier):\n",
    "             addr.append(i)\n",
    "    ##  This addr list condains all the matched words from address identifier list.\n",
    "    ## Now it's time to remove the identified addresses after getting their position in the text.\n",
    "    \n",
    "    \n",
    "    addr_ind=[]\n",
    "    k=0\n",
    "    ll=len(st)\n",
    "    for i in addr:\n",
    "      while(1):\n",
    "        ind=st.find(str(i),k,ll)\n",
    "        if(ind==-1):\n",
    "          break\n",
    "        if(ind!=0 and ind!=ll):  \n",
    "          if((st[ind-1].isalpha() or st[ind+len(str(i))].isalpha())):\n",
    "            k=ind+len(str(i))\n",
    "          else:\n",
    "            break\n",
    "      addr_ind.append(ind)\n",
    "      k=ind+len(str(i))      \n",
    "   ## Here addr index list contains the positions of the matched words front the address identifier list.\n",
    "\n",
    "    addr_list=[]  \n",
    "    if(addr_ind!=[]):\n",
    "      temp=addr_ind[0]\n",
    "      a=[]\n",
    "      for val in addr_ind:\n",
    "            if(val-temp<20):\n",
    "              a.append(val)\n",
    "              temp=val\n",
    "            else:\n",
    "              addr_list.append(a)\n",
    "              a=[]\n",
    "              a.append(val)\n",
    "              temp=val\n",
    "      addr_list.append(a)  \n",
    "  #  print(addr_list)        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### IN ORDER TO REMOVE THE ADDRESSES THE COMPLETE WORD CONTAINING THE ADDRESS IDENTIFIER WORD(matched from the address identifier list)\n",
    "    #### HAS TO BE REMOVED.\n",
    "    ##   SO THE BELOW CODE GETS THE SPAN OF THE COMPLETE WORD IN WHICH THE ADDRESS IDENTIFIER WORD WAS USED.\n",
    "    add_list=[]\n",
    "    for a in addr_list:\n",
    "        flag=[]\n",
    "        jj=a[0]\n",
    "        while(st[jj] not in [',','\\n','.',';'] and jj!=-1):\n",
    "            jj=jj-1\n",
    "        strt=jj\n",
    "        ind1=strt\n",
    "        count=8\n",
    "        while(count and jj !=-1 and st[jj]!='\\n'):\n",
    "          if(st[jj].isdigit()):\n",
    "            strt=jj\n",
    "          jj=jj-1\n",
    "          count=count-1\n",
    "        jj=a[-1]\n",
    "        while(st[jj] not in [',','\\n','.',';'] and jj!=-1):\n",
    "          jj=jj+1\n",
    "        end=jj\n",
    "        ind2=end\n",
    "        count=7\n",
    "        while(count and jj !=ll and st[jj]!='\\n'):         # ll len(st)\n",
    "          if(st[jj].isdigit()):\n",
    "            end=jj\n",
    "          jj=jj+1\n",
    "          count=count-1\n",
    "        if((st[ind1]!='.' or st[ind2]!='.') and (ind2-ind1)<50):\n",
    "          if(st[strt]=='\\n'):\n",
    "                strt=strt+1\n",
    "          if(st[end]=='\\n'):\n",
    "                end=end-1\n",
    "          flag.append(st[strt:end+1])\n",
    "          flag.append(strt)\n",
    "          flag.append(end)\n",
    "          add_list.append(flag)    \n",
    "   ### After the above code executes it gives the complete span of the word which needs to be removed in order to hide the \n",
    "   ### address information\n",
    "    \n",
    "    for a in add_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "\n",
    "\n",
    "   ### After covering a lot of area we are now left with names,dates which could not be identified by refular expression\n",
    "   ### and age.\n",
    "      \n",
    "        \n",
    "        \n",
    "    ## to extract the dates which could not be identified by regular expression we are using en_core_web_sm spacy language model\n",
    "    ## some manipulations neede to be done in order to make the default model works according to our requirments.\n",
    "    doc3=nlp(st)\n",
    "    date_list2=[]\n",
    "    for ents in doc3.ents:\n",
    "                if(str(ents.text).count('X')<2):\n",
    "                    date=[]\n",
    "                    if(ents.label_=='DATE' and (sum([True if i not in st[ents.start_char:ents.end_char].upper() else False for i in time])==len(time)) and (ents.end_char-ents.start_char)>4 and sum(c.isdigit() for c in st[ents.start_char:ents.end_char])>=1 and sum(c.isalpha() for c in st[ents.start_char:ents.end_char])>=1):\n",
    "                        date.append(ents.text)\n",
    "                        date.append(ents.start_char)\n",
    "                        date.append(ents.end_char)\n",
    "                        date_list2.append(date)\n",
    "\n",
    "    \n",
    "    for a in date_list2:\n",
    "        count=0\n",
    "        for i in range(a[1],a[1]+4):\n",
    "            if(st[i].isnumeric()):\n",
    "                count=count+1\n",
    "        if(count==4):\n",
    "            st=st[:a[1]+4]+'X'*(a[2]-a[1]-4)+st[a[2]:]\n",
    "        else:\n",
    "            count=0\n",
    "            for j in range(a[2],a[2]-5,-1):                                ## remvoing the year so that\n",
    "                if(st[j].isnumeric()):                                     ## the year is left untouched.\n",
    "                    count=count+1                                          ##\n",
    "            if(count==4):\n",
    "                st=st[:a[1]]+'X'*(a[2]-4-a[1])+st[a[2]-4:]\n",
    "            elif(count==3):\n",
    "                st=st[:a[1]]+'X'*(a[2]-2-a[1])+st[a[2]-2:]\n",
    "            else:\n",
    "                st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:] \n",
    "    \n",
    "    date_list=[]\n",
    "    if(choice==1):\n",
    "        for a in date_list1:\n",
    "            date_list.append(a)\n",
    "    for a in date_list2:\n",
    "        date_list.append(a)\n",
    "    ### This date_list contains all the dates we extracted including the regular expression portion and spacy default model portion\n",
    "    \n",
    "   \n",
    "\n",
    "    ### Finally lets go for age.\n",
    "    ### To identify age we are again using spacy style regex matcher(phrasematcher) which takes as input patterns we want to match\n",
    "    ### and outputs the start index and end index of the matched pattern.\n",
    "    \n",
    "    ### The following line of code extracts age from text and check weather the extracted age is >89 .If yes remove it else leave as it is.\n",
    "    try:\n",
    "      age_list=[]\n",
    "      matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "      age_indicator=['YEAR', 'YEARS', 'Y/O', 'AGES', 'AGE', 'Y.O', 'Y.O.','AGED','AGE IS']\n",
    "      matcher.add(\"age\", None, nlp3(\"76 year old\"),nlp3(\"aged 58\"),nlp3('aged 123'),nlp3(\"54 y/o\"),nlp3(\"age is 59\"),nlp3(\"123 y/o\"), nlp3(\"ages 35\"),nlp3(\"age 45\"),nlp3(\"ages 123\"),nlp3(\"age 123\"),nlp3(\"54 years old\"),nlp3(\"124 years old\"),nlp3(\"41 y.o.\"),nlp3(\"123 y.o.\"),nlp3('113 year old'))\n",
    "      doc = nlp3(st)\n",
    "      for match_id, start, end in matcher(doc):\n",
    "          if(sum([True if i in str(doc[start:end]).upper() else False for i in age_indicator])>=1):\n",
    "              a=[]\n",
    "              for i in range(start,end):\n",
    "                  if(str(doc[i:i+1]).isnumeric()):\n",
    "                      if(int(str(doc[i:i+1]))>89):\n",
    "                          result=st.find(str(doc[start:end]))\n",
    "                          count=0\n",
    "                          for j in range(result,result+len(str(doc[start:end]))):\n",
    "                                  if(st[j:j+1].isnumeric() and count==0):\n",
    "                                      strt=j\n",
    "                                  if(st[j:j+1].isnumeric()):\n",
    "                                      count=count+1\n",
    "                          a.append(st[strt:strt+count])   \n",
    "                          a.append(strt)\n",
    "                          a.append(strt+count)\n",
    "                          age_list.append(a)\n",
    "                          st=st[:strt]+'X'*count+st[strt+count:]     \n",
    "    except:\n",
    "      None                           \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Finally lets pack all the extracted pattern in a dictionary with key as name of pattern and value as list of list contining\n",
    "    ### matached pattern ,start_index,end_index.\n",
    "    d={}\n",
    "    d['date']=date_list\n",
    "    d['mail']=mail_list\n",
    "    d['aadhar']=aadhar_list\n",
    "    d['ip']=ip_list\n",
    "    d['url']=url_list\n",
    "    d['license_plate']=license_plate_list\n",
    "    d['phone_fax']=phone_fax_list\n",
    "    d['account_serialno']=account_and_serial_list\n",
    "    d['pan']=pan_list\n",
    "    d['passport']=passport_list\n",
    "    d['credit_card']=credit_card_list\n",
    "    d['age']=age_list\n",
    "    d['address']=add_list\n",
    "    d['shifted_date']=shifted_datelist\n",
    "    d['medical_report_no']=medical_rep\n",
    "    shift=random_val \n",
    "    \n",
    "    ## Returning the processed string with all the information hidden,along with dictionary containing them and if choice was 2\n",
    "    ## the shift(by which all the dates are shifted) has to be returned as well ortherwise return shift as None(i.e for choice 1)\n",
    "    if(choice==1):\n",
    "        return(st,d,None)\n",
    "    else:\n",
    "        return(st,d,shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing all the required dependencies.\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "from datetime import timedelta\n",
    "import spacy\n",
    "import re\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import random\n",
    "from random import randint\n",
    "import pickle\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "## we have 2 pickle file\n",
    "## data : containg the terms assosiated with medical fields.\n",
    "## data2 : containg the names of indian cities\n",
    "## These are basically used as lookup table to reduce the error\n",
    "with open('whitelist.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "with open('city_state_list.pkl', 'rb') as f:\n",
    "    data2 = pickle.load(f)\n",
    "data2.append('mumbai')\n",
    "\n",
    "## loading spacy en_core_web_sm language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "## loading a re-trained spacy language model on medical data\n",
    "nlp2=spacy.load('trained_spacy_model2')\n",
    "nlp3=English()\n",
    "\n",
    "### function takes string and choice as input and reutns the processed text along with dictionary of extracted information and shift\n",
    "def master(string,choice):\n",
    "    ## deidentifier function is called which returns processed string,dictionary and shift\n",
    "    st,dic,shift=deidentifier(string,nlp,nlp3,choice)  ## 2 for shifted dates.1 to remove them completely\n",
    "    ## the string we get here has almost all the information hidden except one last remaining part. That is name of persons and organisation.\n",
    "    ## To extract names from the processed text we are using re-trained spacy model.\n",
    "    \n",
    "    ## The below lines of code extract the names of person and org. from the processed text and hides them also.\n",
    "    tokenizer = Tokenizer(nlp3.vocab)\n",
    "    doc2=nlp2(st)\n",
    "    person_org_list=[]\n",
    "    for ents in doc2.ents:\n",
    "        if(str(ents.text).count('X')<3):\n",
    "          tokens=tokenizer(str(ents.text))\n",
    "          if(sum([True if str(i).lower() in data or '\\n' in str(i) or str(i).lower() in data2 else False for i in tokens])!=len(tokens)):\n",
    "                     a=[]\n",
    "                     a.append(ents.text)\n",
    "                     a.append(ents.start_char)\n",
    "                     a.append(ents.end_char)   \n",
    "                     person_org_list.append(a)\n",
    "    dic['person_and_org']=person_org_list\n",
    "    for a in person_org_list:\n",
    "      st=st[:a[1]]+'X'*(a[2]-a[1])+st[a[2]:]\n",
    "    \n",
    "    ## final processed string,dictionay and shift is returned.\n",
    "    return(st,dic,shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing1.txt\n"
     ]
    }
   ],
   "source": [
    "## input the name of text file to be processed.\n",
    "final_str,dic=master(input(),2)  ## 2 for shifted dates.1 to remove them completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date []\n",
      "mail []\n",
      "aadhar [['8193 1847 0840', 120, 134]]\n",
      "ip []\n",
      "url []\n",
      "license_plate []\n",
      "phone_fax [['7182947593', 146, 156]]\n",
      "account_serialno []\n",
      "pan []\n",
      "passport []\n",
      "credit_card []\n",
      "age []\n",
      "address [['Flat 408, Tagore Road Hostel ', 3634, 3662], ['Tagore Road ', 3664, 3675]]\n",
      "shifted_date [['2091-08-12', 14, 24], ['2091-08-15', 172, 182], ['2091-08-12', 264, 274], ['2091-07-16', 460, 470], ['2091-08-16', 3557, 3567]]\n",
      "medical_report_no [['9814048', 94, 101], ['7999590', 3581, 3588], ['1748004', 3623, 3630]]\n",
      "person_and_org [['HARSHA KUMARI', 53, 66], ['Neha Patel', 217, 227], ['Kokilaben Dhirubhai Ambani Hospital', 2007, 2042], ['Neha Patel', 3534, 3544]]\n"
     ]
    }
   ],
   "source": [
    "## extracted information.\n",
    "for items,val  in dic.items():\n",
    "    print(items,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Record date: 2091-08-12\n",
      "\n",
      "Visit Note\n",
      "\n",
      "\n",
      "Patient Name: XXXXXXXXXXXXX                       MRN: XXXXXXX\n",
      "Aadhaar Card No.: XXXXXXXXXXXXXX\n",
      "Phone No.: XXXXXXXXXX\n",
      "Dictated Date: 2091-08-15                      Dictated By: XXXXXXXXXX, M.D.\n",
      "Place: Mumbai\n",
      "\n",
      "Date of Visit: 2091-08-12\n",
      "\n",
      "\n",
      "\n",
      "The patient is a 45 year old woman with a history of diabetes, hypertension, coronary artery disease who comes in today for followup. Please see the recent note from cardiology from 2091-07-16. She is status post stenting in 03/2091 and 06/2091. Post her second procedure she also had a large hematoma which has been gradually resolving. \n",
      "\n",
      "\n",
      "She has type 2 diabetes and her fasting sugars have been very well controlled. She is currently on metformin 1000 mg b.i.d. and glyburide 5 mg b.i.d. She is trying to watch her diet and salt intake. She has not had any symptoms of hypoglycemia. She had lost some weight after her first heart attack but her weight has been stable. \n",
      "\n",
      "\n",
      "Her lisinopril was recently increased by cardiology. She has no exertional chest pain, PND, orthopnea, edema, or claudication. \n",
      "\n",
      "\n",
      "She has a history of iron deficiency anemia and had refused colonoscopies in the past. At the time of her heart attack she had been intubated and had some coffee grounds noted and also required a blood transfusion. She has not noted any epigastric pain, abdominal pain, melena, or hematochezia. She does take her iron supplements and her stools are dark on the iron supplements. I had discussed with patient and daughter several times that she should get a colonoscopy to evaluate her guaiac iron deficiency anemia but she continues to refuse one. \n",
      "\n",
      "\n",
      "Also noted on her admissions was that she had a lung nodule and possible pancreatic mass on CT from 03/2091 and 06/2091. She should get a repeat CT in 12/2091. \n",
      "\n",
      "\n",
      "She is not having any abdominal pain. \n",
      "\n",
      "\n",
      "She has chronic back pain and osteoarthritis pain. Of note she also had elevated parathyroid levels in the past but refuses to see Endocrinology either in Lucknow or at XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX, Mumbai. \n",
      "\n",
      "\n",
      "Please see her medication list which was reviewed with the patient and her daughter. \n",
      "\n",
      "\n",
      "PHYSICAL EXAMINATION: She is in no acute distress. Blood pressure 132/70. Neck: Supple without JVD or bruits. Lungs: Clear without wheezes, rales, or rhonchi. Cardiac: Rate regularly regular, normal S1 and S2. No S3, S4 or bruits. Abdomen: Mildly obese, soft, nontender. She still has some residual hematoma near her right groin. She has a left femoral artery bruit. The hematoma has significantly decreased in size compared to prior visit. No CCE. Distal pulses intact. \n",
      "\n",
      "\n",
      "1. Coronary artery disease. She has no symptoms of decompensated heart failure on her current regimen. Her blood pressure is well controlled with the recent increase in the lisinopril from 20 to 40 mg daily. Check Chem-7. \n",
      "\n",
      "\n",
      "Hypomagnesemia. Check followup magnesium level. \n",
      "\n",
      "\n",
      "Type 2 diabetes. Her last hemoglobin A1c was 6.1 which is excellent. Continue current medications. \n",
      "Iron deficiency anemia. She should have a colonoscopy +/- an upper endoscopy but she declines these procedures. She is aware that we have not ruled out colon cancer or adenomatous colonic polyps or other cause for her iron deficiency anemia. Check CBC. \n",
      "\n",
      "\n",
      "History of lung nodule and possible pancreatic mass versus IPMN. These were both stable on imaging from 03/2091 and 06/2091. She needs repeat imaging in 6 months. She will follow up with me in 10/2091 so we can get a flu shot and schedule these procedures. \n",
      "__________________________\n",
      "XXXXXXXXXX, M.D.\n",
      "\n",
      "\n",
      "TD: 2091-08-16 04:38:30\n",
      "TR: XXXXXXX\n",
      "BackJob ID:  411287\n",
      "VoiceJob ID:  XXXXXXX8\n",
      "\n",
      "\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXX \n",
      "XXXXXXXXXXX \n",
      "New Delhi \n",
      "110002 \n",
      "INDIA \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## final processed string\n",
    "print(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
