{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/harshal/Documents/Projects/GE/speech_to_text/deepspeech-0.6.1-models'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "deepspeech\n",
    "wave\n",
    "json\n",
    "numpy\n",
    "scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harshal/Documents/Projects/GE/speech_to_text/deepspeech-0.6.1-models\n"
     ]
    }
   ],
   "source": [
    "cd deepspeech-0.6.1-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeech import Model\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import wave\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "BEAM_WIDTH = 500\n",
    "LM_ALPHA = 0.75\n",
    "LM_BETA = 1.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_from_metadata(metadata):\n",
    "    word = \"\"\n",
    "    word_list = {}\n",
    "    word_start_time = 0\n",
    "    # Loop through each character\n",
    "    for i in range(0, metadata.num_items):\n",
    "        item = metadata.items[i]\n",
    "        # Append character to word if it's not a space\n",
    "        if item.character != \" \":\n",
    "            if len(word) == 0:\n",
    "                # Log the start time of the new word\n",
    "                word_start_time = item.start_time\n",
    "\n",
    "            word = word + item.character\n",
    "        # Word boundary is either a space or the last character in the array\n",
    "        if item.character == \" \" or i == metadata.num_items - 1:\n",
    "            word_duration = item.start_time - word_start_time\n",
    "\n",
    "            if word_duration < 0:\n",
    "                word_duration = 0\n",
    "\n",
    "            \n",
    "            each_word = {}\n",
    "            each_word['start_time'] = round(word_start_time, 4)\n",
    "            each_word['duration'] = round(word_duration, 4)\n",
    "            \n",
    "#             each_word[\"start_time\"] = {round(word_start_time, 4)}\n",
    "#             each_word[\"duration\"] = round(word_duration, 4)\n",
    "            if word not in word_list.keys():\n",
    "                word_list[word]=[]\n",
    "            word_list[word].append(each_word)\n",
    "            # Reset\n",
    "            word = \"\"\n",
    "            word_start_time = 0\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framerate:  16000\n",
      "Infering /home/harshal/Documents/Projects/GE/speech_to_text/audio/test_2.wav file\n",
      "a hand was isa\n"
     ]
    }
   ],
   "source": [
    "dirPath = \"/home/harshal/Documents/Projects/GE/speech_to_text/\"\n",
    "pathToAudioFile = \"audio/test_2.wav\"\n",
    "newpath = \"audio/test_2_deid.wav\"\n",
    "\n",
    "model_name = dirPath + \"deepspeech-0.6.1-models/output_graph.pbmm\"\n",
    "langauage_model = dirPath + \"deepspeech-0.6.1-models/lm.binary\"\n",
    "trie = dirPath + \"deepspeech-0.6.1-models/trie\"\n",
    "audio_file = dirPath + pathToAudioFile\n",
    "audio_deid = dirPath + newpath\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ds = Model(model_name, BEAM_WIDTH)\n",
    "    ds.enableDecoderWithLM(langauage_model, trie, LM_ALPHA, LM_BETA)\n",
    "    \n",
    "    #Read original audio file\n",
    "    file_obj = open(audio_file,'rb')\n",
    "    fin = wave.open(file_obj)\n",
    "    fs = fin.getframerate()\n",
    "    print(\"Framerate: \", fs)\n",
    "\n",
    "    audio = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)\n",
    "\n",
    "    audio_length = fin.getnframes() * (1/sample_rate)\n",
    "    timestamp = words_from_metadata(ds.sttWithMetadata(audio))\n",
    "    fin.close()\n",
    "    file_obj.close()\n",
    "\n",
    "    print(\"Infering {} file\".format(audio_file))\n",
    "    transcript = ds.stt(audio)\n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2974166666666667"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': [{'start_time': 0.0, 'duration': 0.54}],\n",
       " 'was': [{'start_time': 0.66, 'duration': 1.64}],\n",
       " 'a': [{'start_time': 2.48, 'duration': 0.12},\n",
       "  {'start_time': 4.56, 'duration': 0.66},\n",
       "  {'start_time': 6.52, 'duration': 0.0}],\n",
       " 'lad': [{'start_time': 2.64, 'duration': 1.62}],\n",
       " 'is': [{'start_time': 5.34, 'duration': 0.9}]}"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deidentified_file(input_audio_file_path, timestamp, phi_word_list, fs=16000):\n",
    "    #Read file as a numpy array\n",
    "    from scipy.io.wavfile import read\n",
    "    a = read(input_audio_file_path)\n",
    "    arr = np.array(a[1],dtype=np.int16)\n",
    "    #Each word in given list\n",
    "    for word in phi_word_list:\n",
    "        #Each instance of the word\n",
    "        for i in range(len(timestamp[word])):\n",
    "            #Get start position in array\n",
    "            start = int(timestamp[word][i]['start_time']*fs)\n",
    "            #Get end position in array\n",
    "            end = int((timestamp[word][i]['duration']*fs) - start)\n",
    "            #Mute the required part\n",
    "            arr[start: end] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = get_deidentified_file(audio_file, timestamp=timestamp, phi_word_list=['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write in new audio file\n",
    "from scipy.io.wavfile import write\n",
    "write(audio_deid, rate=16000, data=array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why should one Halt on the Way'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truecasing_by_pos(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "def truecasing_by_pos(input_text):\n",
    "    # tokenize the text into words\n",
    "    words = nltk.word_tokenize(input_text)\n",
    "    # apply POS-tagging on words\n",
    "    tagged_words = nltk.pos_tag([word.lower() for word in words])\n",
    "    # apply capitalization based on POS tags\n",
    "    capitalized_words = [w.capitalize() if t in [\"NN\",\"NNS\"] else w for (w,t) in tagged_words]\n",
    "    # capitalize first word in sentence\n",
    "    capitalized_words[0] = capitalized_words[0].capitalize()\n",
    "    # join capitalized words\n",
    "    text_truecase = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(capitalized_words))\n",
    "    return text_truecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/harshal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
